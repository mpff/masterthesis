
Let $\beta$ be a continuous planar curve.
It can be represented in a parameterized form in $\mathbb{R}^2$ as
$$ \beta : [0,1] \rightarrow \mathbb{R}^2,\quad \beta(t) = ( x(t), y(t)) \,, $$
where $x, y$ are scalar-valued \textit{coordinate functions} of $\beta$, parametrized by $t$.
We can equivalently represent a planar curve using complex numbers as
$$ \beta : [0,1] \rightarrow \mathbb{C},\quad \beta(t) = x(t) + iy(t) \,, $$
with the added benefit that complex notation often simplifies calculations in the 2D case.

For a set of planar curves $\beta_1,\dots,\beta_n : [0,1] \rightarrow \mathbb{C}$, either centered with $\langle \beta_i, \mathbb{1} \rangle$ or with no relative translation to each other, the \textit{full Procrustes mean} $\hat{\mu}$ is then defined as the curve minimizing the sum of squared \textit{full Procrustes distances} from each $\beta_i$ to an unknown unit size mean configuration $\mu$, that is
\begin{align*}
    \hat{\mu} =& \argmin_{\mu:[0,1]\rightarrow\mathbb{C}} \sum_{i=1}^n d^2_F(\mu,\beta_i)
    \quad\text{s.t.}\,\, ||\mu|| = 1 \\
    =& \argmin_{\mu:[0,1]\rightarrow\mathbb{C}} \sum_{i=1}^n 1 - \frac{\langle \mu, \beta_i \rangle \langle \beta_i, \mu \rangle}{\langle \mu, \mu \rangle \langle \beta_i, \beta_i \rangle}
    \quad\text{s.t.}\,\, ||\mu|| = 1
\end{align*}
which we can be further simplified by normalizing $\beta_i := \frac{\beta_i}{|| \beta_i ||}$ and using $\langle \mu, \mu \rangle = 1$
$$ \hat{\mu} = \argmax_{\mu:[0,1]\rightarrow\mathbb{C}} \sum_{i=1}^n \langle \mu, \beta_i \rangle \langle \beta_i, \mu \rangle \quad\text{s.t.}\,\, ||\mu|| = 1. $$
The expression for $d^2_F(\mu,\beta_i)$ in the case of planar curves is derived in appendix \ref{app:deriv-full-proc-dist}.


\subsection{The Full Procrustes mean}
Consider a set of planar SRV curves $q_1,\dots,q_n : [0,1] \rightarrow \mathbb{C}$ of unit length $||q_i|| = 1$ for all $i$.
The \textit{full Procrustes mean} $\hat{\mu}$ is given by
\begin{align*}
    \hat{\mu} =& \argmax_{\mu:[0,1]\rightarrow\mathbb{C}} \sum_{i=1}^n \langle \mu, q_i \rangle \langle q_i, \mu \rangle
    \quad\text{s.t.}\,\, ||\mu|| = 1 \\
    =& \argmax_{\mu:[0,1]\rightarrow\mathbb{C}} \sum_{i=1}^n
    \int_0^1 \overline{\mu(t)} q_i(t) \, dt \int_0^1 \overline{q_i(s)} \mu(s) \, ds
    \quad\text{s.t.}\,\, ||\mu|| = 1 \\
    =& \argmax_{\mu:[0,1]\rightarrow\mathbb{C}}  \int_0^1 \int_0^1
    \overline{\mu(t)} \underbrace{\left( \sum_{i=1}^n q_i(t) \overline{q_i(s)} \right)}_{\coloneqq \, n \hat{C}(s,t)} \mu(s) \, dt ds
    \quad\text{s.t.}\,\, ||\mu|| = 1 \\
    =& \argmax_{\mu:[0,1]\rightarrow\mathbb{C}} \int_0^1
    \overline{\mu(t)} \int_0^1 \hat{C}(s,t) \mu(s) \, ds dt
    \quad\text{s.t.}\,\, ||\mu|| = 1
\end{align*}
with the solution given by the eigenfunction corresponding to the largest eigenvector of the complex empirical covariance function $\hat{C}(s,t) = n^{-1} \sum_{i=1}^n q_i(t) \overline{q_i(s})$.

\newpage
\subsection{The Full Procrustes Mean in a fixed basis}
To avoid having to sample the estimated covariance surface $\hat{C}(s,t)$ on a large grid when calculating its leading eigenfunction, it might be preferable to calculate this eigenfunction from the vector of basis coefficients directly.
After choosing a basis representation $b = (b_1, \dots, b_k)$ with $b_j : \mathbb{R} \rightarrow \mathbb{R}$ real-valued basis functions, we want to estimate complex coefficients $\theta_j \in \mathbb{C}$ so that the Full Procrustes mean of SRV curves is given by $\hat{\mu}(t) = \sum_{j=1}^k \hat{\theta}_j b_j(t) = b^T \hat{\theta}$:
\begin{align*}
    \hat{\mu} =& \argmax_{\theta : ||b^T\theta||=1} \sum_{i=1}^n \langle b^T\theta, q_i \rangle \langle q_i, b^T\theta \rangle \\
    =& \argmax_{\theta : ||b^T\theta||=1} \sum_{k,l} \sum_{i=1}^n \langle b_k \theta_k, q_i \rangle \langle q_i, b_l \theta_l \rangle \\
    =& \argmax_{\theta : ||b^T\theta||=1} \sum_{k,l} \bar{\theta}_k \theta_l \sum_{i=1}^n \langle b_k, q_i \rangle \langle q_i, b_l \rangle \\
    =& \argmax_{\theta : ||b^T\theta||=1} \theta^H S \theta \\
\end{align*}
where the matrix $S = \left\{ \sum_{i=1}^n \langle b_k, q_i \rangle \langle q_i, b_l \rangle \right\}_{k,l}$ has to be estimated from the observed SRV curves.
We can further simplify $S$ to
\begin{align*}
    S_{kl} =& \sum_{i=1}^n \int_0^1 \bar{b}_k(t) q_i(t) dt \int_0^1 \bar{q}_i(s) b_l(s) ds \\
    =& \int_0^1 \int_0^1 \bar{b}_k(t) \underbrace{\left( \sum_{i=1}^n q_i(t) \bar{q}_i(s) \right)}_{= n \, \hat{C}(s,t)} b_l(s) ds dt\\
    =& n \, \int_0^1 \int_0^1 \bar{b}_k(t) \hat{C}(s,t) b_l(s) ds dt\\
\end{align*}
with $\hat{C}(s,t) = \frac{1}{n} \sum_{i=1}^n q_i(s) \overline{q_i(t)}$ the sample analogue to the complex population covariance function $C(s,t) = \mathbb{E}[q(s)\overline{q(t)}]$.
We may estimate $C(s,t)$ via tensor product splines, so that $\hat{C}(s,t) = \sum_{k,l} \hat{\xi}_{kl} b_k(t) b_l(s)$, where $b_j(t)$, $j=1,\dots,k$ are the same real valued basis functions as used for the mean and $\hat{\xi}_{kl}$ are the estimated complex coefficients.
We can then further simplify $S_{kl}$
\begin{align*}
    S_{kl} =& n \, \int_0^1 \int_0^1 b_k(t) \left( \sum_{p,q} \hat{\xi}_{pq} b_q(t) b_p(s) \right) b_l(s) ds dt\\
    =& n \, \sum_{p,q} \hat{\xi}_{pq} \int_0^1 \int_0^1 b_k(t) b_q(t) b_p(s) b_l(s) ds dt\\
    =& n \, \sum_{p,q} \hat{\xi}_{pq} \langle b_k, b_q \rangle \langle b_p, b_l \rangle\\
    =& n \, \sum_{p,q} \hat{\xi}_{pq} g_{kq} g_{pl}
\end{align*}
where $g_{ij}$, $i,j = 1, \dots, k$ are the elements of the Gram matrix $G = bb^T$ with $G = \mathbb{I}_k$ in the special case of an orthogonal basis.
We can then write the write the matrix $S$ as a function of the estimated coefficient matrix $\hat{\Xi} = (\hat{\xi}_{ij})_{i,j = 1, \dots, k}$ :
\begin{align*}
    S =& n \, G \hat{\Xi} G
\end{align*}
The full Procrustes mean of SRV curves is then given by the solution to the optimization problem
\begin{align*}
    \hat{\mu} =& \argmax_{\theta} n \, \theta^H G \hat{\Xi} G \theta \quad \text{subj. to} \quad ||b^T \theta|| = 1 \\
    =& \argmax_{\theta : ||b^T\theta||=1} \, \theta^H G \hat{\Xi} G \theta \quad \text{subj. to} \quad \theta^H G \theta = 1
\end{align*}
One may solve this by using Lagrange optimization with the Langrangian
\begin{align*}
  \mathcal{L}(\theta,\lambda) =& \, \theta^H G \hat{\Xi} G \theta - \lambda ( \theta^H G \theta - 1)
\end{align*}


\newpage
\subsection{Estimation of the covariance surface $C(s,t)$}
Consider the following model for independent curves
\begin{equation}
    Y_i(t_{ij}) = \mu(t_{ij}, \mathbf{x}_i) + E_i(t_{ij}) + \epsilon(t_{ij}),
    \quad j = 1,\dots,D_i, \, i = 1,\dots,n,
\end{equation}
\textbf{[Fast symmetric additive cov smoothing, skew-symmetry, population vs. sample, etc.]}



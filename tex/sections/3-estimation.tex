\label{sec:3}
\todo[inline]{Create figures for everything.}
Alg. \ref{algo:mean} shows an idealized version of the elastic full Procrustes mean estimation, where it is assumed that each curve $\beta_i$ is fully observed.
This is not the case in practice, as each observation $\beta_i$ is usually itself only observed at a finite number of discrete points $\beta_i(t_{i1}), \dots, \beta_i(t_{im_i})$.
Additionally, the number of observed points per curve $m_i$ might be quite small and the points do not need to follow a common sampling scheme across all curves, a setting which is respectively known as \emph{sparse} and \emph{irregular}.

Following the steps laid out in Alg. \ref{algo:mean}, this section proposes a mean estimation strategy for dealing with sparse and irregular observations.
In a first step, the construction of SRV and warped SRV curves from discrete (and possibly sparse) observations will be shown in Section \ref{sec:3-discrete}.
Section \ref{sec:3-cov} discusses efficient estimation of the complex covariance surface $C(s,t)$ from sparse observations.
In Section \ref{sec:3-mean} the calculation of the leading eigenfunction $u_1$ of $C(s,t)$ in a fixed basis will be shown.
Section \ref{sec:3-pfits} deals with the estimation of the scalar product $\omega = \langle z, \mu_q \rangle$, which gives the optimal rotation and scaling alignment.
Note that the final warping alignment step in Alg. \ref{algo:mean} can be solved by using methods for warping alignment of sparse and irregular curves provided in \cite{Steyer2021}.


\section{Discrete Treatment of SRV Curves}
\label{sec:3-discrete}
A natural first consideration might be how to calculate SRV curves from sparse observations.
We defined the SRV curve of a function $\beta \in \mathcal{AC}([0,1],\,\mathbb{C})$ as $q = \frac{\dot\beta}{\sqrt{\norm{\dot\beta}}}$ (for $\dot\beta \neq 0$).
This means that if we want to calculate the SRV curve, we have to calculate the derivative of $\beta$.
As we never observe the whole function $\beta$ but only a set discrete points $\beta(t_1),\dots,\beta(t_m)$, this is already not straight forward, as we cannot simply calculate a pointwise derivative.
However, following \cite{Steyer2021}, we treat a discretely observed curve $\beta$ as piecewise linear between its observed corners $\beta(t_1),\dots,\beta(t_m)$, which allows us to calculate a piecewise constant derivative on the intervalls $[t_j,\,t_{j+1}]$, $j=1,\dots,m-1$.
Let us first consider the case of unwarped observations.

\paragraph{Initial parameterization}
Usually only the image $\beta(t_1),\dots,\beta(t_m)$, but not the parametrisation $t_1,\dots,t_m$, is observed.
Therefore it is first necessary to construct an initial parameterisation.
A common choice is the \emph{arc-length-parametrisation}, where we set $t_j = \frac{l_j}{l}$ with $l_j = \sum_{k=1}^{j-1} \abs{\beta(t_{k+1}) - \beta(t_k)}$ the polygon-length up to point $j$ for $j \leq 2$, $l_1 = 0$ and $l_m = l$.

\paragraph{Piecewise-constant SRV curve} 
Consider the discrete derivative $\Delta \beta \big\rvert_{[t_j, t_{j+1}]} = \frac{\beta(t_{j+1}) - \beta(t_{j})}{t_{j+1} - t_j}$, which assumes that $\beta$ is linear between its observed corners. 
The corresponding SRV curve $q$ can then be treated as piecewise constant $q\big\rvert_{[t_{j},t_{j+1}]} = q_j$ with $q_j = \Delta \beta \big\rvert_{[t_j, t_{j+1}]} \Big/ \sqrt{\norm{\Delta \beta \big\rvert_{[t_j, t_{j+1}]}}} = \frac{\beta(t_{j+1}) - \beta(t_j)}{\sqrt{t_{j+1} - t_j} \cdot \sqrt{\norm{\beta(t_{j+1}) - \beta(t_j)}}}$ the discrete \emph{square-root-velocity} of $\beta$ between the corners $\beta(t_j)$ and $\beta(t_{j+1})$.

\paragraph{Approximate discrete SRV curve} As shown in \cite{Steyer2021} (cf.\ Fig.\ 3), treating the SRV curves as piecewise-constant functions can lead to overfitting, where the mean shape is estimated too polygon-like.
As an alternative they propose to approximate the derivative, by assuming that it attains the value of the discrete derivative $\Delta \beta \big\rvert_{[t_j,t_{j+1}]}$ at the center $s_j = \frac{t_{j+1} - t_j}{2}$ of the interval $[t_j, t_{j+1}]$.
Using this, we can construct \enquote{approximate observations} $q(s_j) \approx q_j$ of the SRV curve $q$.

\paragraph{Normalization}
We can approximate the normalized SRV curve $z$ using the polygon-length $l$ of $\beta$ by $z_j = q_j \big/ \sqrt{l}$.

\paragraph{} 
\textbf{[TODO: Eventuell den ganzen Part mit den piecewise-constant curves streichen. Am ende wird sowieso nur die Mittelwert-Approximation benutzt.]}

\textbf{[TODO: Nochmal überlegen mit $\gamma^{-1}$ und $\gamma$. Gerade ist das hier glaube ich falsch.]}

Let us now consider a warping function $\gamma$.
The warped discrete derivative is given by $\Delta (\beta \circ \gamma) \big\rvert_{[\gamma(t_j), \gamma(t_{j+1})]} = \frac{\beta(\gamma(t_{j+1})) - \beta(\gamma(t_{j}))}{\gamma(t_{j+1}) - \gamma(t_j)}$.
The corresponding warped SRV curve is then given by  $(q \circ \gamma) \sqrt{\dot\gamma} \big\rvert_{[\gamma(t_j), \gamma(t_{j+1})]} = \frac{\beta(\gamma(t_{j+1})) - \beta(\gamma(t_j))}{\sqrt{\gamma(t_{j+1}) - \gamma(t_j)} \cdot \sqrt{\norm{\beta(\gamma(t_{j+1})) - \beta(\gamma(t_j))}}}$.

\textbf{[TODO: Das hier zuende formulieren.]}

\begin{figure}
  \centering
  \begin{subfigure}{.48\textwidth}
    \centering
    \missingfigure{Sparsely observed digit 3.}
  \end{subfigure}\hfill%
  \begin{subfigure}{.48\textwidth}
    \centering
    \missingfigure{Sparse SRV of digit 3.}
  \end{subfigure}
  \caption{A sparsely observed planar curve (left) with sparse approximate SRV curve (right). Data: see Figure \ref{fig:1-shape}.}
  \label{fig:3-disc}
\end{figure}



\section{Efficient Estimation using Hermitian Covariance Smoothing}
\label{sec:3-cov}

We want to estimate $C(s,t) = \mathbb{E}[z(s)\overline{z(t)}]$ given approximate observations of the normalized SRV curves $z_i(s_{ij})$, with $j = 1,\dots,m_i-1$ and $i=1,\dots,N$, where $m_i$ denotes the number of observed points per curve.
Following \textbf{[TODO: Cite]}, we can treat this estimation as a smoothing problem, by constructing responses $y_{ijk} = z_i(s_{ij}) \overline{z_i(s_{ik})}$ and treating the pairs $s_{ij}$, $s_{ik}$ as covariates $s$ and $t$.
Smooothing the responses $y_{ijk}$ gives an estimate $\hat C(\cdot, \cdot)$ of $C(s,t)$, as each response has expectation $\mathbb{E}[y_{ijk}] = C(s_{ij},s_{ik})$. 
A popular approach \textbf{[TODO: Cite]} is to carry out the smoothing in a \emph{tensor product spline} basis 
$$ C(s,t) = b(s)^T \Xi b(t) $$
where $b(s) = (b_1(s),\dots,b_K(s))$ denotes the vector of a spline basis and $\Xi$ is a $K \times K$ coefficient matrix to be estimated.
As $C(s,t)$ is complex, we restrict the spline basis to be real-valued with $b_k : \mathbb{R} \rightarrow \mathbb{R}$ for $k = 1,\dots,K$ and the coefficient matrix to be complex-valued with $\Xi \in \mathbb{C}^{K \times K}$, without loss of generality.


Considering the symmetry properties of the covariance surface allows for more efficient estimation, as shown in \cite{CederbaumScheiplGreven2018} for real valued, symmetric covariance surfaces, by considering every unique pair $s_{ij}$,$s_{ik}$ only once.
In the complex case, the covariance surface is hermitian with $C(s,t) = \overline{C(t,s)}$, which means we can decompose the estimation into two seperate regression problems over the symmetric real and skew-symmetric imaginary parts of $C(s,t)$.
$$\mathbb{E}[\Re(y)] = b(s)^T \Xi_{\Re} b(t)$$
$$\mathbb{E}[\Im(y)] = b(s)^T \Xi_{\Im} b(t)$$
with $\Xi_\Re, \Xi_\Im \in \mathbb{R}^{K\times K}$ and $\Xi = \Xi_\Re + i \Xi_\Im$, under the constraints that $\Xi_\Re^T = \Xi_\Re$ and $\Xi_\Im^T = - \Xi_\Im$.
In this thesis I estimate $\Xi_\Re$ and $\Xi_\Im$ using the \texttt{R} \parencite{Rcore} package \texttt{mgcv} \parencite{Wood2017}.
For efficient estimation, two \texttt{mgcv} smooths from the package \texttt{sparseFLMM} \parencite{sparseFLMM} are used, which implement and generalize the approach proposed by \cite{CederbaumScheiplGreven2018} for symmetric and skew-symmetric tensor product p-splines.
\textbf{[TODO: Erklärung p-splines?]}
\textbf{[TODO: Motivate b-spline/p-spline basis using Lisa's paper?]}
\textbf{[TODO: REML?]}


\section{Estimating the Elastic Full Procrustes Mean in a Fixed Basis}
\label{sec:3-mean}
To estimate the elastic full Procrustes Mean, we have to solve a functional eigenvalue problem on the estimated covariance surface $\hat{C}(s,t) = b(s)^t \hat\Xi b(t)$.
This may be achieved by evaluating $\hat{C}(s,t)$ on a dense grid and performing an eigendecomposition on the matrix of evaluations \textbf{[TODO: Cite]}.
Alternatively, we can estimate the mean directly in some basis $b(s) = (b_1(s),\dots,b_K(s))$, where a natural choice might be to evaluate mean and covariance surface in the same basis, i.e. $\mu_q(s) = b(s)^T \theta$.

Remember that the elastic full Procrustes mean (for fixed warping) is given by the solution to the optimization problem
$$ \hat\mu_q = \, \argmax_{\mu_q \in \mathbb{L}^2:\,\norm{\mu_q} = 1}\,\,
   \int_0^1 \int_0^1 \overline{\mu_q (s)} C(s,t) \mu_q(t) \,ds\,dt\,\,. $$
Given an estimate of the covariance surface $\hat{C}(s,t) = b(s)^T \hat\Xi b(t)$, the mean estimation then reduces to estimating the vector of coefficients $\theta = (\theta_1, \dots, \theta_K) \in \mathbb{C}^K$ with
\begin{align*}
  \hat\theta& \, = \, \argmax_{\theta \in \mathbb{C}^K:\,\norm{b^T\theta} = 1}\,\,
    \int_0^1 \int_0^1 \theta^H b(s) b(s)^T \hat\Xi b(t) b(t)^T \theta \,ds\,dt \\
  &\, = \, \argmax_{\theta \in \mathbb{C}^K:\,\norm{b^T\theta} = 1}\,\,
    \theta^H \left( \int_0^1 b(s) b(s)^T \,ds \right) \hat\Xi \left( \int_0^1 b(t) b(t)^T \,dt\right) \theta \\
  &\, = \, \argmax_{\theta \in \mathbb{C}^K:\,\theta^H G \theta = 1}\,\,
    \theta^H G \hat\Xi G \theta 
\end{align*}
where $(\cdot)^H = \overline{(\cdot)}^T$ denotes the conjugate transpose and $G$ is the $K \times K$ Gram matrix with entries given by the basis products $g_{ij} = \langle b_i, b_j \rangle$.
In the special case of an orthonormal basis with $\langle b_i, b_j \rangle = \delta_{ij}$ the Gram matrix is an identity matrix, however, this is not the case for many basis representations such as the b-spline basis.

We have reduced the functional eigenvalue problem to a multivariate eigenvalue problem over the covariance coefficient matrix.
We might solve this using Lagrange optimization with the following Langrangian:
\begin{equation*}
  \mathcal{L}(\theta,\lambda) = \, \theta^H G \hat{\Xi} G \theta - \lambda ( \theta^H G \theta - 1)
\end{equation*}
Taking into account that we identified $\mathbb{R}^2$ with $\mathbb{C}$ we can split everything into real and imaginary parts and optimize with respect to $\Re(\theta)$ and $\Im(\theta)$ seperately, to avoid having to take complex derivatives.
Using $\theta = \theta_\Re + i \theta_\Im$ and $\hat{\Xi} = \hat\Xi_\Re + i \hat\Xi_\Im$ we can write
\begin{align*}
  \mathcal{L}(\theta_\Re,\theta_\Im,\lambda) & = \, 
    (\theta_\Re^T - i \theta_\Im^T) G ( \hat{\Xi}_\Re + i \hat\Xi_\Im ) G ( \theta_\Re + i \theta_\Im )
    - \lambda \left( (\theta_\Re^T - i \theta_\Im^T ) G (\theta_\Re + i \theta_\Im) - 1 \right) \\
  & = \, \theta_\Re^T G \hat\Xi_\Re G \theta_\Re 
    + i \theta_\Re^T G \hat\Xi_\Im G \theta_\Re 
    + \theta_\Im^T G \hat\Xi_\Im G \theta_\Re
    - \theta_\Re^T G \hat\Xi_\Im G \theta_\Im \\
  & \qquad  + \theta_\Im^T G \hat\Xi_\Re G \theta_\Im
    + i \theta_\Im^T G \hat\Xi_\Im G \theta_\Im 
    + \lambda \left( \theta_\Re^T G \theta_\Re + \theta_\Im^T G \theta_\Im - 1 \right)
\end{align*}
using $\hat\Xi_\Re^T = \hat\Xi_\Re$ and $\hat\Xi_\Im^T = - \hat\Xi_\Im$.
Differentiation w.r.t.\ $\theta_\Re$ and $\theta_\Im$ yields
\begin{align}
  \frac{\partial \mathcal{L}}{\partial \theta_\Re} & = \, 
    2G\hat\Xi_\Re G \theta_\Re - 2G\hat\Xi_\Im G \theta_\Im - 2\lambda G\theta_\Re \overset{!}{=} 0 \label{eq:lagrRe}\\
  \frac{\partial \mathcal{L}}{\partial \theta_\Im} & = \,
    2G\hat\Xi_\Re G \theta_\Im + 2G\hat\Xi_\Im G \theta_\Re - 2\lambda G\theta_\Im \overset{!}{=} 0 \label{eq:lagrIm}
\end{align}
with the additional constraint $\theta_\Re^T G \theta_\Re + \theta_\Im^T G \theta_\Im = 1$.T
We can simplify this further and multiply Eq. \ref{eq:lagrIm} by $i$, leading to
\begin{align}
  \hat\Xi_\Re G \theta_\Re - \hat\Xi_\Im G \theta_\Im & = \lambda \theta_\Re\\
  i \hat\Xi_\Re G \theta_\Im + i \hat\Xi_\Im G \theta_\Re & = i \lambda \theta_\Im \,.
\end{align}
Adding both equations finally leads to  
$$( \hat\Xi_\Re + i \hat\Xi_\Im) G \theta_\Re + i (\hat\Xi_\Re + \hat\Xi_\Im) G \theta_\Im = \lambda ( \theta_\Re + i \theta_\Im)$$
or likewise, using $\theta$ and $\hat\Xi$
$$ \hat\Xi G \theta = \lambda \theta $$
which is an eigenvalue problem on the product of the complex coefficient matrix and the Gram matrix.
Multiplying by $\theta^H G$ from the left yields $\lambda = \theta^H G \hat\Xi G \theta$, i.e. the eigenvalues correspond to the target function to maximize.
It follows that the estimate for the coefficient vector of the elastic full Procrustes mean is given by the eigenvector of the leading eigenvalue of $\hat\Xi G$.
\textbf{[TODO: Cite Reiss.]}



\section{Numerical Integration of the Procrustes Fits}
\label{sec:3-pfits}
\textbf{[TODO: Mean value theorem in the integral, etc.]}

$\hat{p}(t) = b(t)^T \hat{\theta}$ estimated Procrustes mean function, $q$ piecewise constant SRV transform with $q|_{[t_j, t_{j+1}]} = q_j \in \mathbb{C}$.

For estimation of the Procrustes fits we need to estimate two scalar products:

$$ \hat{q}_{p} = \frac{\langle q,\hat{p} \rangle}{\langle q, q \rangle} q $$

So far I treat $q$ as piecewise constant (as in Lisa's paper):

$$ \langle q, \hat{p} \rangle = \int_0^1 \langle q(t), \hat{p}(t) \rangle \, dt \approx \sum_{j=0}^{m-1} \int_{t_j}^{t_{j+1}} \langle q_j, \hat{p}(t) \rangle \, dt $$

$$ \langle q, q \rangle = \int_0^1 \langle q(t), q(t) \rangle \, dt \approx \sum_{j=0}^{m-1} (t_{j+1} - t_j) \, \langle q_j, q_j \rangle $$


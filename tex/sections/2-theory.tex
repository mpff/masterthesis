\label{sec:2}
As a starting point, it is important to establish a notational and mathematical framework for the treatment of planar shapes.
While the restriction to the 2D case might seem a major one, it still covers all shape data extracted from e.g.\ imagery and is therefore very applicable in practice.
The outline of a 2D object may be naturally represented by a planar curve $\beta : [0,1] \rightarrow \mathbb{R}^2$ with $\beta(t) = (x_1(t),\, x_2(t))^T$, where $x_1(t)$ and $x_2(t)$ are the scalar-valued \textit{coordinate functions}.
Calculations in two dimensions, and in particular the derivation of the full Procrustes mean, are greatly simplified by using complex notation.
\begin{figure}
  \centering
  \begin{subfigure}{.48\textwidth}
    \centering
    \inputTikz{2-curve}
  \end{subfigure}\hfill%
  \begin{subfigure}{.48\textwidth}
    \centering
    \begin{subfigure}{\textwidth}
      \centering
      \inputTikz{2-curveX}
    \end{subfigure}
    \begin{subfigure}{\textwidth}
      \centering
      \inputTikz{2-curveY}
    \end{subfigure}
  \end{subfigure}
  \caption{Example of a planar curve (left) with respective coordinate functions (right) using complex notation. Data: see Figure \ref{fig:1-shape}.}
  \label{fig:2-curve}
\end{figure}
We will therefore identify $\mathbb{R}^2$ with $\mathbb{C}$, as shown in Figure \ref{fig:2-curve}, and always use complex notation when representing a planar curve:
$$\beta : [0,1] \rightarrow \mathbb{C}, \quad \beta(t) = x_1(t) + i\, x_2(t).$$
We will assume the curves to be absolutely continuous, denoted as $\beta \in \mathcal{AC}([0,1], \mathbb{C})$, guaranteeing us that $\beta(t)$ has an integrable derivative.
This is important when working in the square-root-velocity (SRV) framework, as will be discussed in Section \ref{sec:2-dist}.
All considerations will be restricted to the case of open curves, with possible extensions to closed curves $\beta \in \mathcal{AC}(\mathbb{S}^1, \mathbb{C})$ discussed in Section \ref{app:a-closed} of the appendix.


\section{Equivalence Classes and Shape}
\label{sec:2-shape}
As mentioned in the introduction, shape is usually defined by its invariance under the transformations of scaling, translation, and rotation.
When considering the shape of curves, we additionally have to take into account invariance with respect to re-parametrisation.
This can be seen, by noting that the curves $\beta(t)$ and $\beta(\gamma(t))$, with some re-parametrisation or \textit{warping function} $\gamma : [0,1] \rightarrow [0,1]$ monotonically increasing and differentiable, have the same image and therefore represent the same geometrical object (see Figure \ref{fig:1-warp}).
We can say that the actions of translation, scaling, rotation, and re-parametrisation are \textit{equivalence relations} with respect to shape, as each action leaves the shape of the curve untouched and only changes the way it is represented.
The shape of a curve can then be defined as the respective \textit{equivalence class}, i.e. the set of all possible shape preserving transformations of the curve.
As two equivalence classses are neccessarily either disjoint or identical, we can consider two curves as having the same shape, if they are elements of the same equivalence class \parencite[see][40]{SrivastavaKlassen2016}.

When defining an equivalence class, one has to first consider how each individual transformation acts on a planar curve $\beta : [0,1] \rightarrow \mathbb{C}$.
This is usually done using the notion of \textit{group actions} and \textit{product groups}, with the later desciribing multiple transformations acting at once.
A brief introduction to group actions may be found in \cite[Chap.\ 3]{SrivastavaKlassen2016}.

\begin{itemize}[leftmargin=0.75cm]
  \item[1.] The \emph{translation} group $\mathbb{C}$ acts on $\beta$ by $(\xi, \beta) \xmapsto{\text{Trl}} \beta + \xi$, for any $\xi \in \mathbb{C}$.
    We can consider two curves as equivalent with respect to translation $\beta_1 \overset{\text{Trl}}{\backsim} \beta_2$, if there exists a complex scalar $\widetilde\xi \in \mathbb{C}$ so that $\beta_1 = \beta_2  + \widetilde\xi$.
    Then, for some function $\beta$, the related equivalence class with respect to translation is given by $[\beta]_{\text{Trl}} = \{\beta + \xi\, |\, \xi \in \mathbb{C}\}$.
  \item[2.] The \emph{scaling} group $\mathbb{R}^+$ acts on $\beta$ by $(\lambda, \beta) \xmapsto{\text{Scl}} \lambda \beta$, for any $\lambda \in \mathbb{R}^+$.
    We define $\beta_1 \overset{\text{Scl}}{\backsim} \beta_2$, if there exists a scalar $\widetilde\lambda \in \mathbb{R}^+$ so that $\beta_1 = \widetilde\lambda \beta_2$.
    An equivalence class is $[\beta]_{\text{Scl}} = \{\lambda\beta\,|\, \lambda \in \mathbb{R}^+\}$.
  \item[3.] The \emph{rotation} group $[0,2\pi]$ acts on $\beta$ by $(\theta, \beta) \xmapsto{\text{Rot}}  e^{i\theta} \beta$, for any $\theta \in [0,2\pi]$.
    We define $\beta_1 \overset{\text{Rot}}{\backsim} \beta_2$, if there exists a $\widetilde\theta \in [0,2\pi]$ with $\beta_1 = e^{i\widetilde\theta} \beta_2$.
    An equivalence class is $[\beta]_{\text{Rot}} = \{e^{i\theta}\beta\,|\, \theta \in [0,2\pi]\}$.
  \item[4.] The \emph{warping} group $\Gamma$ acts on $\beta$ by $(\gamma,\beta) \xmapsto{\text{Wrp}} \beta \circ \gamma$, for any $\gamma \in \Gamma$ with $\Gamma$ being the set of monotonically increasing and differentiable warping functions.
    We define $\beta_1 \overset{\text{Wrp}}{\backsim} \beta_2$, if there exists a warping function $\widetilde\gamma \in \Gamma$ with $\beta_1 = \beta_2 \circ \widetilde\gamma$.
    An equivalence class is $[\beta]_{\text{Wrp}} = \{\beta \circ \gamma\,|\, \gamma \in \Gamma\}$.
\end{itemize}
In a next step, we can consider how these transformations act in concert and whether they \textit{commute}, i.e.\ whether the order of applying the transformations changes outcomes.
Consider for example the actions of the \emph{rotation and scaling} product group $\mathbb{R}^+ \times [0,2\pi]$ given by $\left((\lambda, \theta), \beta\right) \xmapsto{\text{Scl} + \text{Rot}} \lambda e^{i\theta} \beta$, which clearly commutes as $\lambda(e^{i\theta}\beta) = e^{i\theta}(\lambda\beta)$.
On the other hand, the joint actions of \emph{scaling and translation} do not commute, as $\lambda(\beta + \xi) \neq \lambda\beta + \xi$, with the same holding for the joint actions of \emph{rotation and translation}.
As the order of translating and rotating or scaling  matters, one usually takes the translation to act on the already scaled and rotated curve.
The joint action defined using this ordering is called an \emph{Euclidean similarity transformation} with $\left((\xi, \lambda, \theta), \beta\right) \xmapsto{\text{Eucl}} \lambda e^{i\theta} \beta + \xi$ \parencite[see][62]{DrydenMardia2016}.
Considering the action of \emph{warping} or re-parametrization, we can note that it necessarily commutes with all Euclidean similarity transformations as those only act on the image of $\beta$, while the former only acts on the parametrization.
Putting everything together we can give a formal definition of the shape of a planar curve as the following equivalence class:
\begin{definition}[Shape]
  The \emph{shape} of an absolutely continous planar curve $\beta \in \mathcal{AC}([0,1], \mathbb{C})$ is given by its equivalence class $[\beta]$ with respect to all Euclidean similarity transformations and re-parametrizations
  $$ [\beta] = \left\{\lambda e^{i\theta}(\beta \circ \gamma) + \xi\,|\, \xi \in \mathbb{C},\, \lambda \in \mathbb{R}^+,\, \theta \in [0,2\pi],\, \gamma \in \Gamma\right\}. $$
  The \emph{shape space} $\mathcal{S}$ is then given by $\mathcal{S} = \left\{[\beta]\,|\,\beta \in \mathcal{AC}([0,1],\, \mathbb{C})\right\}$. 
\end{definition}


\section{The Elastic Full Procrustes Distance for Planar Curves}
\label{sec:2-dist}
Let us now turn to the construction of an appropriate \emph{shape distance} $d([\beta_1], [\beta_2])$ for two curves $\beta_1$, $\beta_2$.
As the shapes $[\beta_1]$ and $[\beta_2]$ are elements of a non-Euclidean quotient space (the shape space $\mathcal{S}$), calculating a distance between them is already not straight-forward.
A common approach is to map each equivalence class $[\beta]$ to a suitable representative, so that the distance calculation in shape space can be identified with a (much simpler) distance calculation over the representatives in the underlying functional space.

To illustrate this, let us first discuss each type of shape-preserving transformation individually, starting with the Euclidean similarity transformations.
Consider two equivalence classes with respect to translation $[\beta_1]_\text{Trl}, [\beta_2]_\text{Trl}$.
They might be uniquely mapped to their centered elements $\widetilde\beta^\text{Trl}_i = \beta_i - \overline{\beta}_i \in [\beta_i]_\text{Trl}$ for $i=1,2$.
We can then define an appropriate distance that is invariant under translation as $d_\text{Trl}([\beta_1]_\text{Trl}, [\beta_2]_\text{Trl}) = \norm{\widetilde\beta^\text{Trl}_1 - \widetilde\beta^\text{Trl}_2}$.
Similarly, a distance that is invariant under scaling might be defined over the normalized elements $\widetilde\beta^\text{Scl}_i = \frac{\beta_i}{\norm{\beta_i}} \in [\beta_i]_\text{Scl}$ for $i = 1,2$, as $d_\text{Scl}([\beta_1]_\text{Scl}, [\beta_2]_\text{Scl}) = \norm{\widetilde\beta^\text{Scl}_1 - \widetilde\beta^\text{Scl}_i}$.
When considering invariance under rotation, we can first note that no \enquote{standardization} procedure compareable to normalizing and centering exists for the case of rotation.
Instead of mapping $[\beta]_\text{Rot}$ to a fixed representative, we therefore have to identify an appropriate representative on a case-by-case basis.
This can be achieved by defining the distance as the minimal distance $d_\text{Rot}([\beta_1]_\text{Rot}, [\beta_2]_\text{Rot}) = \min_{\widetilde\beta^\text{Rot}_2 \in [\beta_2]_\text{Rot}} \norm{\beta_1 - \widetilde\beta^\text{Rot}_2} = \min_{\theta \in [0,2\pi]} \norm{\beta_1 - e^{i\theta}\beta_2}$, when keeping one curve fixed and rotationally aligning the other curve \parencite[compare e.g][]{Stoecker2021}.


\subsubsection*{The Full Procrustes Distance}
The three approaches can be combined to formulate the two \emph{Procrustes} distances, which are invariant under all Euclidean similarity transforms.
The \emph{partial Procrustes distance} is defined as the minimizing distance $d_{PP}([\beta_1]_\text{Eucl}, [\beta_2]_\text{Eucl}) = \min_{\theta \in [0,2\pi]} \,\, \norm{\widetilde\beta_1 - e^{i\theta} \widetilde\beta_2}$, when rotationally aligning the centered and normalized curves $\widetilde\beta_i= \frac{\beta_i - \overline\beta_i}{\norm{\beta_i - \overline\beta_i}}$, $i=1,2$.
On the other hand, the \emph{full Procrustes distance} (Def. \ref{def:2-fpdist}) includes an additional alignment over scaling, leading to a slightly different geometrical interpretation \parencite[see][77-78]{DrydenMardia2016}.
In this thesis we will only consider the full Procrustes distance, although no distance definition is inherently better than the other.
In the context of mean estimation for sparse and irregular curves, the full Procrustes distance might be slightly more suitable, as the additional scaling alignment offers more flexibility in a setting where calculating a norm $\norm{\beta} = \int_0^1 \norm{\beta(t)}\, dt$ may already present a challange.
Note that in Def. \ref{def:2-fpdist} the optimization over scaling $\lambda \in \mathbb{R}$ and rotation $\theta \in [0,2\pi]$ was combined into a single optimization over \emph{rotation and scaling} $\omega = \lambda e^{i\theta} \in \mathbb{C}$.
Furthermore, Fig. \ref{fig:2-pfit} shows an example of two curves that where aligned by minimizing their full Procrustes distance.
\begin{definition}[Full Procrustes distance]
  \label{def:2-fpdist}
  The \emph{full Procrustes distance} for two equivalence classes $[\beta_1]_\text{Eucl}$, $[\beta_2]_\text{Eucl}]$ is defined as
  \begin{equation}
    d_{FP}([\beta_1]_\text{Eucl}, [\beta_2]_\text{Eucl}) = \min_{\omega \in \mathbb{C}} \,\, \norm{\widetilde\beta_1 - \omega \widetilde\beta_2}
  \end{equation}
  with centered and normalized representatives $\widetilde\beta_i = \frac{\beta_i - \overline\beta_i}{\norm{\beta_i - \overline\beta_i}}$.
\end{definition}

\begin{lemma}
  \label{lem:2-fpdist}
  Let $\beta_1, \beta_2 : [0,1] \rightarrow \mathbb{C}$ be two planar curves with corresponding equivalence classes $[\beta_1]_\text{Eucl}, [\beta_2]_\text{Eucl}$ with respect to Euclidean similarity transforms and let $\widetilde\beta_i = \frac{\beta_i - \overline\beta_i}{\norm{\beta_i - \overline\beta_i}}$.
  \begin{enumerate}[label=\emph{\roman*.)}]
    \item The full Procrustes distance between $[\beta_1]_\text{Eucl}$ and $[\beta_2]_\text{Eucl}$ is given by 
      \begin{equation}
        d_{FP}([\beta_1]_\text{Eucl},[\beta_2]_\text{Eucl}) = \sqrt{ 1 - \langle \widetilde\beta_1, \widetilde\beta_2 \rangle \langle \widetilde\beta_2, \widetilde\beta_1 \rangle }
      \end{equation}
    \item The optimal rotation and scaling alignment of $\widetilde\beta_2$ onto $\widetilde\beta_1$ is given by $\omega^\text{opt} = \langle \widetilde\beta_2, \widetilde\beta_1 \rangle$. 
      The aligned curve $\widetilde\beta_2^{P} = \langle \widetilde\beta_2, \widetilde\beta_1 \rangle \cdot \widetilde\beta_2$ is then called the \emph{Procrustes fit} of $\widetilde\beta_2$ onto $\widetilde\beta_1$.
  \end{enumerate}
  \begin{proof}
    See Appendix \ref{app:a-deriv-fpdist}.
  \end{proof}
\end{lemma}

\begin{figure}[!hbt]
  \centering
  \begin{subfigure}{.48\textwidth}
    \centering
    \inputTikz{2-pfit}
  \end{subfigure}\hfill%
  \begin{subfigure}{.48\textwidth}
    \centering
    \inputTikz{2-pfit-aligned}
  \end{subfigure}
  \caption{
    Procrustes fit (right; normalized and centered) of two example curves (left).
    The Procrustes fit of $\beta_2$ (green) onto $\beta_1$ (blue) is given by $\widetilde\beta_2^P = \langle \widetilde\beta_2,\, \widetilde\beta_1 \rangle \widetilde\beta_2$.
    Data: See Figure \ref{fig:1-shape}.}
  \label{fig:2-pfit}
\end{figure}


\subsubsection*{The Elastic Distance}
When considering warping, we would like to do something similar to rotation by trying to find an optimal warping alignment between two curves $\beta_1$, $\beta_2$ by optimizing over their distance $\inf_{\gamma \in \Gamma} \norm{\beta_1 - (\beta_2 \circ \gamma)}$, where $\Gamma$ is the space of warping functions.
The usual choice is to optimize this over the $\mathbb{L}^2$-distance between the curves, but in this case the result would not define a proper distance.
Optimizing over re-parametrization using the $\mathbb{L}^2$-distance has problems relating to the so called \textit{pinching effect} and \textit{inverse-inconsistency}, where the later means that aligning the parametrisation of one curve to another by $\inf_{\gamma \in \Gamma} \norm{\beta_1 - \beta_2 \circ \gamma}$ may yield different results than $\inf_{\gamma \in \Gamma} \norm{\beta_2 - \beta_1 \circ \gamma}$ \parencite[see][88\psq]{SrivastavaKlassen2016}.
A solution proposed by \cite{SrivastavaEtAl2011} is to ditch the $\mathbb{L}^2$-metric in favor of an \textit{elastic metric}, which is isometric with respect to re-parametrization.
Calculation of this metric, the Fisher-Rao Riemannian metric \parencite{Rao1945}, can be greatly simplified by using the \textit{square-root-velocity} (SRV) framework, as the Fisher-Rao metric of two curves can be equivalently calculated as the $\mathbb{L}^2$-distance of their respective SRV curves.
As this SRV representation makes use of derivatives, any curve $\beta$ that has a SRV curve must fulfill some kind of differentiability constraint.
Here it is enough to consider only curves that are absolutely continuous $\beta \in \mathcal{AC}([0,1],\, \mathbb{C})$, which in particular means that the original curves do not have to be smooth but might also be piecewise linear \parencite[see][91]{SrivastavaKlassen2016}.
Note that because of the use of derivatives, any elastic analysis of curves will automatically be translation invariant as well.
\begin{definition}[Elastic distance \parencite{SrivastavaEtAl2011}]
  \label{def:2-eldist}
  The \emph{elastic distance} of two equivalence classes $[\beta_1]_{\text{Wrp} + \text{Trl}}$, $[\beta_2]_{\text{Wrp} + \text{Trl}}$ is defined as
  \begin{equation}
    d_{E}([\beta_1]_\text{Wrp+Trl}, [\beta_2]_\text{Wrp+Trl}) = \inf_{\gamma \in \Gamma} \,\, \norm{q_1 - ( q_2 \circ \gamma) \cdot \sqrt{\dot\gamma}}
  \end{equation}
  with centered and normalized representatives $\widetilde\beta_i = \frac{\beta_i - \overline\beta_i}{\norm{\beta_i - \overline\beta_i}}$.
  The \emph{square-root-velocity} (SRV) representation of an absolutely continuous planar curve $\beta \in \mathcal{AC}([0,1],\,\mathbb{C})$ is given by 
  $$ \quad q(t) = 
    \begin{cases}
      \frac{\dot{\beta}(t)}{\sqrt{\norm{\dot{\beta}(t)}}} & \dot{\beta}(t) \neq 0 \\
      0 & \dot{\beta}(t) = 0
    \end{cases}\,, $$
  where $q \in \mathbb{L}^2([0,1], \mathbb{C})$ and $\dot\beta(t) = \frac{\partial\beta(t)}{\partial t}$.
\end{definition}

\begin{figure}
  \centering
  \begin{subfigure}{.48\textwidth}
    \centering
    \inputTikz{2-srv}
  \end{subfigure}\hfill%
  \begin{subfigure}{.48\textwidth}
    \centering
    \begin{subfigure}{\textwidth}
      \centering
      \inputTikz{2-srvX}
    \end{subfigure}
    \begin{subfigure}{\textwidth}
      \centering
      \inputTikz{2-srvY}
    \end{subfigure}
  \end{subfigure}
  \caption{SRV function (left) of the planar curve in Figure \ref{fig:2-curve} with respective SRV coordinate functions (right). Note that the polygon-like look of the SRV curve is an artifact of the (linear) smoothing applied to the original data on SRV level (see Appendix \ref{app:a-smooth}). Data: see Figure \ref{fig:1-shape}.}
  \label{fig:2-srv}
\end{figure}

\todo[inline]{More detail about bijective between $\mathcal{AC}$ and $\mathbb{L}^2$!}
\begin{remark}
The original curve $\beta$ can be re-constructed from its SRVF, up to translation, by $\beta(t) = \beta(0) + \int_0^t q(s) \norm{q(s)} ds$.
\end{remark}
The SRVFs are considered elements of a Hilbert space, which is given by $\mathbb{L}^2([0,1],\,\mathbb{C})$ equipped with the complex inner product $\langle \cdot, \cdot \rangle$ and corresponding norm $\norm\cdot$.
The complex inner product of $q,q' \in \mathbb{L}^2([0,1],\,\mathbb{C})$ is defined as
$$ \langle q, q' \rangle = \int_0^1 \overline{q(t)} q'(t) dt \,, $$
with $\overline{z} = \operatorname{Re}(z) - i \operatorname{Im}(z)$ denoting the complex conjugate.


As we can always recover the original curve up to translation, the SRV representation holds all relevant information about the shape of a curve.
Furthermore, because of the use of derivatives, the SRV representation is invariant under changes in translation of the original curve.
As a consequences, all shape-preserving transformations commute on SRV level.

\begin{lemma}
  The actions of the translation, scaling, rotation, and re-parametrization groups commute on SRV level.
\begin{proof} The SRVF $\widetilde q(t)$ of  $\widetilde\beta(t) = \lambda e^{i\theta}\beta\left(\gamma(t)\right) + \xi$ is given by
$$ \widetilde q (t) 
  = \frac{\lambda e^{i\theta} \dot\beta\left(\gamma(t)\right) \dot\gamma(t)}{\sqrt{||\lambda e^{i\theta} \dot\beta\left(\gamma(t)\right) \dot\gamma(t)||}} 
  = \sqrt{\lambda} e^{i\theta} \frac{\dot\beta\left(\gamma(t)\right)}{\sqrt{||\dot\beta\left(\gamma(t)\right)||}} \sqrt{\dot\gamma(t)} 
  = \sqrt\lambda e^{i\theta} \left( q \circ \gamma \right) \sqrt{\dot\gamma(t)}.$$
The result is irrespective of the order of applying the transformations.
\end{proof}
\end{lemma}
\begin{remark}
  It follows, that the individual transformations translate to SRV level by 
  $$\text{i.)} \,\,(\xi, q) \xmapsto{\text{Trl}} q,\quad 
    \text{ii.)} \,\, (\lambda, q) \xmapsto{\text{Scl}} \sqrt\lambda q,\quad
    \text{iii.)} \,\, (\theta, q) \xmapsto{\text{Rot}} e^{i\theta} q,\quad
    \text{iv.)} \,\, (\gamma, q) \xmapsto{\text{Wrp}} (q \circ \gamma) \sqrt{\dot\gamma}.$$
\end{remark}
Going forward, we will now work in the SRV framework and use the elastic metric for distance calculations between shapes.
This means, instead of optimizing over the $\mathbb{L}^2$ distance between the original curves, we optimize over the $\mathbb{L}^2$ distance between their respective SRVFs.
For two absolutely continuous curves $\beta_1, \beta_2 \in \mathcal{AC}([0,1],\,\mathbb{C})$ with respective SRV curves $q_1, q_2 \in \mathbb{L}^2([0,1],\,\mathbb{C})$ we might define the \textit{elastic} distance between their shapes as
$$d([\beta_1], [\beta_2]) = 
  \inf_{\widetilde q_1 \in [q_1],\,\widetilde q_2 \in [q_2]} \norm{ \widetilde q_1 - \widetilde q_2 }_{\mathbb{L}^2}$$
or when equivalently optimizing over all possible transformations as
$$d([\beta_1], [\beta_2]) = 
  \inf_{\lambda_{1,2} \in \mathbb{R}^+,\,\theta_{1,2} \in [0,2\pi],\, \gamma_{1,2} \in \Gamma} \norm{ \sqrt{\lambda_1} e^{i\theta_1}(q_1 \circ \gamma_1) \sqrt{\dot\gamma_1} - \sqrt{\lambda_2} e^{i\theta_2}(q_2 \circ \gamma_2) \sqrt{\dot\gamma_2}}_{\mathbb{L}^2}\,.$$

We can simplify the optimization, by considering that both rotation and warping act by isometries on the elastic metric, which means in practice it is enough to optimize over the rotation and warping of only one of the curves.
For warping, we can reformulate the optimization as a problem over the relative parametrisation between the curves, as $\inf_{\gamma_1,\gamma_2 \in \Gamma} \norm{(q_1 \circ \gamma_1) \sqrt{\dot\gamma_1} - (q_2 \circ \gamma_2) \sqrt{\dot\gamma_2}} = \inf_{\gamma_1,\gamma_2 \in \Gamma} \norm{q_1 - (q_2 \circ (\gamma_2 \circ \gamma_1^{-1})) \sqrt{\dot{(\gamma_2 \circ \gamma_1^{-1})}}}= \,\,\,\,\inf_{\gamma \in \Gamma} \,\,\,\, \norm{q_1 - (q_2 \circ \gamma) \sqrt{\dot\gamma}}$.
And smilarly for rotation, we can optimize over the relative rotation between both curves, as
$\inf_{\theta_1,\theta_2 \in [0,2\pi]} \norm{e^{i\theta_1}q_1 - e^{i\theta_2}q_2} = \inf_{\theta_1, \theta_2 \in [0,2\pi]} \norm{q_1 - e^{i(\theta_2 - \theta_1)}q_2} ) = \inf_{\theta \in [0,2\pi]} \norm{q_1 - e^{i\theta}q_2}$.
Taken together these lead to
$$d([\beta_1], [\beta_2]) = 
  \inf_{\lambda_{1,2} \in \mathbb{R}^+,\,\theta \in [0,2\pi],\, \gamma \in \Gamma} \norm{ \sqrt{\lambda_1}q_1 - \sqrt{\lambda_2} e^{i\theta}(q_2 \circ \gamma) \sqrt{\dot\gamma}}\,,$$
which, however, still has the problem of not being \todo{Distance cannot be isometric} isometric with respect to scaling.

A possible solution, mirroring the definition of the \emph{full Procrustes distance} for landmark data, is to work with the normalized representations $z = \frac{q}{\norm q}$, while only aligning the scaling of one of the curves.
This leads to a distance that is isometric with respect to scaling, as for $\widetilde{q}_{1,2} = \lambda q_{1,2}$ with $\lambda \in \mathbb{R}^+$ it holds that $\norm{\widetilde z_1 - \widetilde z_2} = \norm{ \frac{\lambda q_1}{\norm{\lambda q_1}} - \frac{\lambda q_2}{\norm{\lambda q_2}}} = \norm{z_1 - z_2}$, while also being inverse consistent, as $\inf_{\lambda \in \mathbb{R}^+} \norm{z_1 - \lambda z_2}^2 = \inf_{\lambda \in \mathbb{R}^+}\,\norm{z_1}^2 + \lambda^2 \norm{z_2}^2 - \lambda (\langle z_1, z_2 \rangle + \langle z_2, z_1 \rangle) \overset{\norm{z_{1,2}} = 1}{=} \inf_{\gamma \in \Gamma} \norm{z_2}^2 + \lambda^2 \norm{z_1}^2 - \lambda (\langle z_1, z_2 \rangle + \langle z_2, z_1 \rangle) = \inf_{\lambda \in \mathbb{R}^+} \norm{z_2 - \lambda z_1}^2$.
We can finally take everything together and define an \emph{elastic full Procrustes distance}.
\begin{definition}[Elastic full Procrustes distance]
  \label{def:dist}
  The \emph{elastic full Procrustes distance} between the shapes $[\beta_1]$, $[\beta_2]$ of two continuously differentiable planar curves $\beta_1$, $\beta_2 \in \mathcal{AC}([0,1],\mathbb{C})$  is given by 
    $$d_{EF}([\beta_1], [\beta_2]) = \inf_{\lambda \in \mathbb{R}^+,\, \theta \in
    [0,2\pi],\, \gamma \in \Gamma} \norm{z_1 - \lambda e^{i\theta} (z_2 \circ \gamma) \sqrt{\dot\gamma}}\, $$
    with normalized SRV representation $z_{i} = \frac{q_{i}}{\norm{q_{i}}} \in \mathbb{L}^2([0,1], \mathbb{C})$, where $q_i$ is the SRVF of $\beta_i$, $i = 1,2$.
\end{definition}
\begin{remark}
  If the original curve $\beta$ is of unit length $L[\beta] = \int_0^1 \abs{\dot\beta(t)}dt = 1$, the SRV curve $q = \frac{\dot\beta}{\norm{\dot\beta}}$ will be automatically normalized, as $ \norm{q} = \sqrt{ \int_0^1 |q(t)|^2 \, dt} = \sqrt{\int_0^1 |\dot{\beta}(t)| \, dt} = 1$.
\end{remark}

\todo[inline]{Make duality of approach more explicit!}
To calculate the distance, we need to solve the joint optimization problem over $\Gamma \times \mathbb{R}^+ \times [0,2\pi]$.
For a fixed $\gamma \in \Gamma$, the optimization problem in Definition \ref{def:dist} mirrors the full Procrustes distance for landmark data, where an explicit solution is known in the planar case \parencite[see][Chapter~8]{DrydenMardia2016}.
Likewise, for fixed rotation $\theta \in [0,2\pi]$ and scaling $\lambda \in \mathbb{R}^+$, there are known optimization techniques dealing with re-parametrisation.
\begin{equation*}
  (\lambda^*,\, \theta^*,\, \gamma^*) = \argmin_{\lambda \in \mathbb{R}^+,\, \theta \in [0,2\pi],\, \gamma \in \Gamma} \norm{z_1 - \lambda e^{i\theta} (z_2 \circ \gamma) \sqrt{\dot\gamma}}
\end{equation*}
Here, using one parameter $\omega = \lambda e^{i\theta} \in \mathbb{C}$ for rotation and scaling can simplify notation.
The rotation and scaling parameters can always be recovered by $\lambda = \abs{\omega}$ and $\theta = \arg(\omega)$.
\begin{equation}
  \label{eq:argmin}
  (\omega^*,\, \gamma^*) = \argmin_{\omega \in \mathbb{C},\, \gamma \in \Gamma} \norm{z_1 - \omega (z_2 \circ \gamma) \sqrt{\dot\gamma}}
\end{equation}

The usual strategy is to optimize over the sets of parameters individually and then to iterate through both solutions.
\todo{Citation, write more here!}
Let us first consider the optimization with respect to rotation and scaling.
For fixed $\gamma \in \Gamma$ with $\widetilde z_2 = (z_2 \circ \gamma) \sqrt{\dot\gamma}$ we can write Eq.\ \ref{eq:argmin} as
\begin{equation}
  \label{eq:argmin_rot}
  \omega^* = \argmin_{\omega \in \mathbb{C}}\,\, \norm{z_1 - \omega \widetilde z_2},
\end{equation}
which can be solved analytically.

\begin{lemma}
  \label{lem:dist}
  \begin{enumerate}[label=\emph{\roman*.)}]
    \item For a fixed $\gamma \in \Gamma$, the optimal scaling and rotation solving Eq.\ \ref{eq:argmin_rot} is 
      $$ \omega^* = \langle \widetilde z_2,\, z_1 \rangle = \langle (z_2 \circ \gamma) \sqrt{\dot\gamma},\, z_1 \rangle$$
    \item The optimization problem in Definition \ref{def:dist} can then be reduced to
      $$ d_{EF}([\beta_1],[\beta_2]) = \inf_{\gamma \in \Gamma} \sqrt{ 1 - \langle z_1, (z_2 \circ \gamma) \sqrt{\dot\gamma} \rangle \langle (z_2 \circ \gamma) \sqrt{\dot\gamma}, z_1 \rangle }$$
  \end{enumerate}
  \begin{proof}
  See \ref{app:deriv-full-proc-dist} in the appendix.
  \end{proof}
\end{lemma}
\begin{remark}
  For fixed $\gamma \in \Gamma$, we can use the first part of Lemma \ref{lem:dist} to calculate the optimal rotation and scaling alignment of $\widetilde z_2 = (z_2 \circ \gamma) \sqrt{\dot\gamma}$ onto $z_1$. The alignment $\widetilde z_2^P = \langle \widetilde z_2,\, z_1 \rangle \, \widetilde z_2$ is called the \emph{Procrustes fit} of $\widetilde z_2$ onto $z_1$. The second part of Lemma \ref{lem:dist} will be useful for mean calculation in the next section.
\end{remark}

\begin{figure}
  \centering
  \begin{subfigure}{.32\textwidth}
    \centering
    \inputTikz{2-pfit}
  \end{subfigure}\hfill%
  \begin{subfigure}{.32\textwidth}
    \centering
    \inputTikz{2-pfit-srv-aligned}
  \end{subfigure}\hfill%
  \begin{subfigure}{.32\textwidth}
    \centering
    \inputTikz{2-pfit-aligned}
  \end{subfigure}
  \caption{
    Procrustes fit of two example curves (left) on normalized SRV (middle) and original curve level (right).
    On normalized SRV level, the Procrustes fit of $z_2$ (green) onto $z_1$ (blue) is calculated as $\omega^* z_2$, where $\omega^* = \left\langle z_2,\, z_1 \right\rangle$ is the optimal scaling and rotation alignment.
    On original curve level, the Procrustes fit can then be constructed as 
    Data: see Figure \ref{fig:1-shape}.}
  \label{fig:2-pfit}
\end{figure}

Given the current $\omega \in \mathbb{C}$ with $z_2^P = \omega z_2$, the optimization with respect to re-parametrization can be written as
\begin{equation}
  \label{eq:argmin_wrp}
  \gamma^* = \argmin_{\gamma \in \Gamma}\,\, \norm{z_1 - (z_2^P \circ \gamma) \sqrt{\dot\gamma}}.
\end{equation}
This is a well known problem and usually solved numerically by minimizing a cost function $H[\gamma] = \int_0^1 \norm{z_1(t) - z_2^P\left( \gamma(t) \right) \sqrt{\dot\gamma(t)}}\, dt$ using a dynamic programming algorithm (DPA) or gradient based methods \parencite[see][]{SrivastavaEtAl2011}.
In this thesis we will use the methods laid out in \cite{Steyer2021}, for solving Eq.\ \ref{eq:argmin_wrp} in the setting of sparse and irregularly sampled curves.

\paragraph{Anmerkung für Lisa}
Ich habe mich noch gefragt ob man anstatt Iteration über Eq.\ \ref{eq:argmin_rot} und Eq.\ \ref{eq:argmin_wrp} die Distanz in Lemma \ref{lem:dist} ii.) auch direkter optimieren kann.
\begin{align*}
  \gamma^* = & \argmin_{\gamma \in \Gamma} \sqrt{1 - \langle z_1, \widetilde z_2 \rangle \langle \widetilde z_2, z_1 \rangle } \\
    = & \argmax_{\gamma \in \Gamma} \langle z_1, \widetilde z_2 \rangle \langle \widetilde z_2, z_1 \rangle \\
    = & \argmax_{\gamma \in \Gamma} \int_0^1 \int_0^1
    \overline{z_1(t)} \underbrace{\widetilde z_2(t) \overline{\widetilde z_2(s)}}_{\coloneqq \, {\widetilde C}(s,t)} z_1(s) \, dt ds \\
    = & \argmax_{\gamma \in \Gamma} \langle \widetilde C \, z_1, z_1 \rangle
\end{align*}
Mit $\widetilde C(s,t)$ der Kovarianz-Funktion von $\widetilde z_2 = (z_2 \circ \gamma) \sqrt{\dot\gamma}$.
Habe das jetzt nicht weiter verfolgt (weil Zeit), aber vielleicht ist das ganz interessant?
Man kann das glaube ich auch umschreiben mit $\breve z_1 = (z_1 \circ \gamma^{-1}) \sqrt{\dot{\gamma^{-1}}}$ als $\argmax_{\gamma \in \Gamma} \langle C \, \breve z_1, \breve z_1 \rangle$.

\todo[inline]{Write iterative procedure as algorithm! Basically as in \cite{SrivastavaEtAl2011}.}


\section{The Elastic Full Procrustes Mean for Planar Curves}
\label{sec:2-mean}
\todo[inline]{Nummerierung der Equations überarbeiten}
We now want to calculate shape means for a random sample of planar curves.
Again, we assume all curves to be absolutely continous $\beta_i \in \mathcal{AC}([0,1],\, \mathbb{C})$ with corresponding SRV curves $q_i \in \mathbb{L}^2([0,1],\, \mathbb{C})$, $i=1,\dots,N$.
As we want to take into account shape invariance in the mean calculation, we cannot simply use sums or integrals to calculate a sample mean shape.
Instead, we can use a more general concept for mean calculation, where the mean is defined as a minimizer over the sum of squared distances to each observation, for choice of a sensible distance in the space of interest.
If the resulting mean is a global minimum, it is usually called a \enquote{sample Fr\'echet mean} \parencite{Frechet1948}, if it is a local minimum a \enquote{sample Karcher mean} \parencite{Karcher1977} \parencite[see][111]{DrydenMardia2016}.

\begin{definition}[Sample elastic full Procrustes mean]
  \label{def:mean}
  For a set of curves $\beta_i \in \mathcal{AC}([0,1],\, \mathbb{C})$, $i = 1,\dots,N$,  their \emph{sample elastic full Procrustes mean} is given by the minimizing shape $\hat{[\mu]}$ with
  $$ \hat{[\mu]} = \arginf_{[\mu] \in \mathcal{S}} \sum_{i=1}^N d_{EF}([\mu], [\beta_i])^2\,,$$
  where $\mathcal{S} = \left\{ [\beta] : \beta \in \mathcal{AC}([0,1],\,\mathbb{C}) \right\}$ is the shape space.
\end{definition}

Instead of working with equivalence classes as in Def. \ref{def:mean}, it is often simpler to work with a specific element $\hat\mu \in \hat{[\mu]}$, that acts as a \enquote{representation} of the sample mean shape.
One possibility is to use a representation that is of unit-length and starts at the origin, so that $L[\hat\mu] = \int_0^1 \norm{\dot{\hat\mu}(t)}\, dt = 1$ and $\hat\mu(0) = 0$.
This is an attractive choice when working in the SRV framework, as we do not have to worry about reconstructing translation when calculating $\hat\mu$ from its respective SRV curve $\hat\mu_q$ by $\hat\mu(t) = \hat\mu(0) + \int_0^t \hat\mu_q(s) \norm{\hat\mu_q(s)}\,ds$.
Another possibility would be to use a unit-length representation that is centered, so that $\abs{\int_0^1 \hat\mu(t)\, dt} = 0$, which may be achieved by setting $\hat\mu(0) = \int_0^1 \int_0^t \hat\mu_q(s) \norm{ \hat\mu_q(s)}\,ds\,dt$ when reconstructing $\hat\mu$ from $\hat\mu_q$.
From the point of shape analysis, the choice of representation does not make a difference, as both mean curves are elements of $\hat{[\mu]}$ and therefore have the same shape.
However, the distinction becomes important when the estimated mean curve $\hat\mu$ is used in concert with other curves, for example when visualizing multiple curves or when comparing multiple class mean shapes, as those do not typically share the same center or starting point.
Differences between both representations will be explored using empirical data of tounge shapes in Section \ref{sec:tounge}.

Note that we can always construct $\hat\mu$ (and therefore $\hat{[\mu]}$) from $\hat\mu_q$ by integration.
Turning back to the actual mean calculation, we can therefore use the Def. \ref{def:dist} to reformulate the optimization problem in Def. \ref{def:mean} on SRV level, where we now optimize over possible normalized SRV representations $\mu_q$ of the mean shape $[\mu]$. 
$$ \hat\mu_q = \argmin_{\mu_q \in \mathbb{L}^2:\,\norm{\mu_q} = 1}\,\,
  \sum_{i=1}^N \, \left( \inf_{\omega_i \in \mathbb{C}, \gamma_i \in \Gamma} \,
    \norm{\mu_q - \omega_i (z_i \circ \gamma_i) \sqrt{\dot\gamma_i}} \right)^2 $$
Here, $z_i = \frac{q_i}{\norm{q_i}}$ are the observed normalized SRV curves, $\omega_i$ the rotation and scaling alignment to $\mu_q$ and $\gamma_i$ the re-parametrisation alignment to $\mu_q$.
We can further simplify this by solving the optimization over $\omega_i$ using Lemma \ref{lem:dist} ii.
\begin{align*}
  \hat\mu_q = &\, \argmin_{\mu_q \in \mathbb{L}^2:\,\norm{\mu_q} = 1}\,\,
    \sum_{i=1}^N \,\inf_{\gamma_i \in \Gamma}\, \left(1 - \langle \mu_q,\,(z_i \circ \gamma_i) \sqrt{\dot\gamma_i} \rangle \langle (z_i \circ \gamma_i) \sqrt{\dot\gamma_i},\, \mu_q\rangle \right) \\
  \hat\mu_q = &\, \argmax_{\mu_q \in \mathbb{L}^2:\,\norm{\mu_q} = 1}\,\,
    \sum_{i=1}^N \,\sup_{\gamma_i \in \Gamma}\, \langle \mu_q,\,(z_i \circ \gamma_i) \sqrt{\dot\gamma_i} \rangle \langle (z_i \circ \gamma_i) \sqrt{\dot\gamma_i},\, \mu_q\rangle \\
  \hat\mu_q = &\, \argmax_{\mu_q \in \mathbb{L}^2:\,\norm{\mu_q} = 1}\,\,
    \sum_{i=1}^N \,\sup_{\gamma_i \in \Gamma}\, \left\langle \mu_q,\,(\gamma_i,z_i) \right\rangle \left\langle (\gamma_i, z_i), \mu_q \right\rangle
\end{align*}
We end up with a two step optimization problem consisting of an outer optimization over $\mu_q$ and an inner optimization over the set $\left\{\gamma_i\right\}_{i=1,\dots,N}$.
Similarly to the approaches discussed in \cite{SrivastavaKlassen2016} and to \cite{Steyer2021}, we solve this by \emph{template based alignment} \parencite[see e.g.][271]{SrivastavaKlassen2016}.
In a first step the mean $\hat\mu_q$ is estimated while keeping the parametrisations fixed, after which the $\gamma_i$ are updated by pairwise warping-alignment between $z_i$ and $\hat\mu_q$, which is usually achieved by DPA or a gradient based approach.
Both steps are iterated until the mean shape has converged.
\textbf{[TODO: Noch genauer auf warping alignment step eingehen -> muss ja erstmal auch procrustes fit berechnen um warping alignen zu können.]}

Let us now consider the outer optimization problem for a fixed set of warping function $\{\gamma_i^*\}_{i=1,\dots,N}$ where we denote the warping aligned normalized SRV curves $(\gamma_i^*, z_i)$ as $\widetilde{z}_i$.
Note that if no warping alignment has happened yet, we can always set $\gamma_i^*(t) = t$ for all $i=1,\dots,N$ as a starting value.
The problem we have to solve is:
\begin{align*}
  \hat\mu_q = &\, \argmax_{\mu_q \in \mathbb{L}^2:\,\norm{\mu_q} = 1}\,\,
    \sum_{i=1}^N \, \left\langle \mu_q,\, \widetilde z_i \right\rangle \left\langle \widetilde z_i,\, \mu_q \right\rangle \\
  \hat\mu_q = &\, \argmax_{\mu_q \in \mathbb{L}^2:\,\norm{\mu_q} = 1}\,\,
    \sum_{i=1}^N \, \int_0^1 \int_0^1 \overline{\mu_q (s)} \widetilde z_i(s) \overline{\widetilde z_i(t)} \mu_q(t) \,ds\,dt \\
  \hat\mu_q = &\, \argmax_{\mu_q \in \mathbb{L}^2:\,\norm{\mu_q} = 1}\,\,
    \int_0^1 \int_0^1 \overline{\mu_q (s)} \left( \sum_{i=1}^N\, \widetilde z_i(s) \overline{ \widetilde z_i(t)} \right) \mu_q(t) \,ds\,dt
\end{align*}
We can identify the inner term as proportional to a sample estimator $\check{C}(s,t) = \frac{1}{N} \sum_{i=1}^N z_i(s) \overline{z_i(t)}$ of the population covariance surface $C(s,t) = \mathbb{E}[z(s) \overline{z(t)}]$, when noting that $\mathbb{E}[z(t)] = 0$ for all $t \in [0,1]$ due to rotational symmetry.
\textbf{[TODO: Hier wird die Notation etwas unsauber. Man hat hier ja einen Cov. Estimator aus den "gewarpten" Kurven. Vielleicht müsste man sich hier nicht nochmal kurz Gedanken machen wie $C(s,t)$ eigentlich aussieht, wenn man berücksichtigt, dass die $z(t)$ auch random gewarpt sind. Vielleicht ist's auch egal.]}

\begin{figure}
  \centering
  \begin{subfigure}{.48\textwidth}
    \centering
    \missingfigure{Example original Covariance surface (real part)}
  \end{subfigure}\hfill%
  \begin{subfigure}{.48\textwidth}
    \centering
    \missingfigure{Example SRV Covariance surface (real part}
  \end{subfigure}\\
  \begin{subfigure}{.48\textwidth}
    \centering
    \missingfigure{Example original Covariance surface (imaginary part)}
    \caption{Covariance surface on original curve level.}
  \end{subfigure}\hfill%
  \begin{subfigure}{.48\textwidth}
    \centering
    \missingfigure{Example SRV Covariance surface (imaginary part}
    \caption{Covariance surface on SRV curve level.}
  \end{subfigure}
  \caption{Complex covariance surface on original and SRV curve level. Data: see Figure \ref{fig:1-shape}.}
  \label{fig:2-cov}
\end{figure}

$$ \hat\mu_q = \, \argmax_{\mu_q \in \mathbb{L}^2:\,\norm{\mu_q} = 1}\,\,
    N\cdot\int_0^1 \int_0^1 \overline{\mu_q (s)} \check C(s,t) \mu_q(t) \,ds\,dt $$
By replacing $\check C(s,t)$ by its expectation $C(s,t)$, we can analogously formulate an estimator on the population level.
\textbf{[TODO: Notation?]}
$$ \hat\mu_q = \, \argmax_{\mu_q \in \mathbb{L}^2:\,\norm{\mu_q} = 1}\,\,
    \int_0^1 \int_0^1 \overline{\mu_q (s)} C(s,t) \mu_q(t) \,ds\,dt $$

We can rewrite this again as a functional scalar product by considering the \emph{covariance operator} $C$ with $(C\mu_q)(s) = \int_0^1 C(s,t) \mu_q(t) dt$ \parencite[see][153]{RamsaySilverman2005}.
\begin{equation}
  \label{eq:quadr_opt}
  \hat\mu_q = \, \argmax_{\mu_q \in \mathbb{L}^2,\,\norm{\mu_q} = 1}\,\,
    \left\langle \mu_q, C\mu_q \right\rangle
\end{equation}
This is a well known problem in the context of functional principal component analysis (FPCA).
From $\overline{C(s,t)} = \overline{\mathbb{E}[z(s)\overline{z(t)}]} = \mathbb{E}[z(t)\overline{z(s)}] = C(t,s)$ it follows that $\left\langle \mu_q, C \mu_q \right\rangle = \left\langle C \mu_q, \mu_q \right\rangle$ and therefore that $C$ is a \emph{self-adjoint} operator.
The optimization problem then reduces to an eigenfunction problem 
\begin{equation}
  \label{eq:funceig}
  Cu = \lambda u \quad \Leftrightarrow \quad \int_0^1 C(s, t) u(t)\, dt = \lambda u(s)\,,
\end{equation}
where $\lambda = \left\langle \mu_q,\, C \mu_q \right\rangle$ is the target function to maximize.
For normalized eigenfunctions $u_1, u_2,\,\dots$ and corresponding eigenvalues $\lambda_1 \geq \lambda_2 \geq \dots$ of $C(s,t)$, the solution $\hat\mu_q(t)$ is given by the leading normalized eigenfunction $u_1(t)$ of $C(s,t)$ \parencite[see][153,397]{RamsaySilverman2005}.

\begin{algorithm}[Sample elastic full Procrustes mean]
  \label{algo:mean}
  Let $\left\{\beta_i\right\}_{i=1,\dots,N}$ be a sample of planar curves with corresponding SRV curves $\left\{ q_i \right\}_{i=1,\dots,N}$.
  Let $z_i = \frac{q_i}{\norm{q_i}}$.
  Set $\gamma_i^0 = t$ for all $i=1,\dots,N$ as the initial parametrisation alignment.
  Set $k = 0$. 
  \begin{enumerate}
    \item For $i=1,\dots,N$: Set $z_i^k(t) = z_i\left(\gamma^k_i(t)\right) \cdot \sqrt{\dot\gamma_i^k(t)}$.
    \item Estimate $C(s,t) = \mathbb{E}[z(s)\overline{z(t)}]$ from $\left\{z_i^k\right\}_{i=1,\dots,N}$. Call this estimate $\hat{C}^k(s,t)$.
    \item Set $\mu_q^k$ as the leading normalized eigenfunction of $\hat{C}^k(s,t)$. \emph{\textbf{Stop}} if $\mu_q^k$ is close to $\mu_q^{k-1}$.
    \item For $i=1,\dots,N$: Calculate the optimal rotation and scaling alignment $\omega^k_i = \left\langle z_i^k,\, \mu_q^k \right\rangle$.
    \item For $i=1,\dots,N$: Solve $\gamma_i^{k+1} = \argmin_{\gamma \in \Gamma} \norm{\mu_q^k - (\omega_i^k \cdot z_i \circ \gamma ) \sqrt{\dot\gamma}}$.
    \item Set $k = k+1$ and return to Step 1.
  \end{enumerate}
\end{algorithm}


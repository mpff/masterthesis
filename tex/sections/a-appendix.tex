\label{app:a}

\section{Additional Proofs and Derivations}
\label{app:a-deriv}

\subsection{Derivation of Lemma \ref{lem:2-fpdist}}
\label{app:a-deriv-fpdist}
\begin{align*}
  d_{FP}([\beta_1]_\Eucl,[\beta_2]_\Eucl)^2 
    = & \min_{\omega \in \mathbb{C}} \norm{\widetilde \beta_1 - \lambda e^{i\theta} \widetilde \beta_2}^2 \\
    = & \min_{\lambda \in \mathbb{R}^+,\, \theta \in [0,2\pi]} \norm{\widetilde \beta_1 - \lambda e^{\iu\theta} \widetilde \beta_2}^2 \\
    = & \min_{\lambda \in \mathbb{R}^+,\, \theta \in [0,2\pi]} \langle \widetilde \beta_1 - \lambda e^{\iu\theta} \widetilde \beta_2, \widetilde \beta_1 - \lambda e^{\iu\theta} \widetilde \beta_2 \rangle \\
    = & \min_{\lambda \in \mathbb{R}^+,\, \theta \in [0,2\pi]} \norm{\widetilde \beta_1}^2 + \lambda^2 \norm{\widetilde \beta_2}^2 - \lambda ( e^{\iu\theta} \langle \widetilde \beta_1, \widetilde \beta_2 \rangle + e^{-\iu\theta} \langle \widetilde \beta_2, \widetilde \beta_1 \rangle 
\end{align*}
Define $\langle \widetilde \beta_1, \widetilde \beta_2 \rangle = \kappa e^{\iu\phi} \in \mathbb{C}$ and use $\norm{\widetilde \beta_1} = \norm{\widetilde \beta_2} = 1$.
\begin{align*}
  d_{FP}([\beta_1]_\Eucl,[\beta_2]_\Eucl)^2 
   = &  \min_{\lambda \in \mathbb{R}^+,\, \theta \in [0,2\pi]} 1 + \lambda^2  - \lambda ( e^{\iu\theta} \kappa e^{\iu\phi} + e^{-\iu\theta} \kappa e^{-\iu\phi} ) \\
   = &  \min_{\lambda \in \mathbb{R}^+,\, \theta \in [0,2\pi]} 1 + \lambda^2  - \lambda \kappa \left( e^{\iu(\theta + \phi)} + e^{-\iu (\theta + \phi)} \right) \\
   = &  \min_{\lambda \in \mathbb{R}^+} 1 + \lambda^2  - \max_{\theta \in [0,2\pi]} 2 \lambda \kappa \cos{(\theta + \phi)} \\
   \overset{\theta^\mathrm{opt} = -\phi}{=} & \min_{\lambda \in \mathbb{R}^+} 1 + \lambda^2  - 2 \lambda \kappa
\end{align*}
From $\frac{\partial}{\partial\lambda}\left( 1 + \lambda^2 - 2\lambda\kappa \right) = 2\lambda - 2\kappa \overset{!}{=} 0$ it follows that $\lambda^\mathrm{opt} = \kappa$.
\begin{align*}
  d_{FP}([\beta_1]_\Eucl,[\beta_2]_\Eucl)^2 
   = &  \left( 1 + \kappa^2  - 2 \kappa^2 \right) =  \left( 1 - \kappa^2 \right)
\end{align*}
\cref{lem:2-fpdist} i.) follows by considering $\kappa^2 = \abs{\langle \widetilde \beta_1, \widetilde \beta_2 \rangle}^2 = \langle \widetilde \beta_1, \widetilde \beta_2 \rangle \langle \widetilde \beta_2, \widetilde \beta_1 \rangle$.
Then
\begin{align*}
  d_{FP}([\beta_1]_\Eucl,[\beta_2]_\Eucl)
   = &  \sqrt{ 1 - \langle \widetilde \beta_1, \widetilde \beta_2 \rangle \langle \widetilde \beta_2, \widetilde \beta_1 \rangle }\,.
\end{align*}
\cref{lem:2-fpdist} ii.) follows by $\omega^\mathrm{opt} = \lambda^\mathrm{opt} e^{\iu \theta^\mathrm{opt}} = \kappa e^{-\iu\phi} = \overline{\langle \widetilde \beta_1, \widetilde \beta_2 \rangle} = \langle \widetilde \beta_2, \widetilde \beta_1 \rangle$.


\section{Discussion of Possible Extensions to Closed Curves}
\label{app:a-closed}


\section{Shape-Smoothing Using the Estimated Covariance-Surface}
\label{app:a-smooth}
Idea: Smooth $q$ in mean basis $b(t)$, so that $\hat{q}(t) = b(t)^T \hat{\theta}_q$. Then the scalar products simplify to
$$ \langle q, \hat{p} \rangle \approx \hat{\theta}_q^H G \hat{\theta} $$
$$ \langle q, q \rangle \approx \hat{\theta}_q^H G \hat{\theta}_q $$

As $q$ can be sparse, we may want to use the estimated covariance matrix $\hat{\Xi}$ when estimating $\theta_q$ in the form of a Normal prior $\theta_q \sim \mathcal{N}_{\mathbb{C}^k}(0, \hat\Xi)$ leading to

$$ \hat\theta_q = (B^T B + \Xi^{-1})^{-1} B^T q $$

or equivalently using eigendecompositon $\Xi = V \Lambda V^{-1}$ with $V^H V = \mathcal{I}$ and $\Lambda = diag(\lambda_1, \dots, \lambda_k)$

$$ \hat\theta_q = V ( V^H B^T B V + \Lambda^{-1})^{-1} V^H B^T q$$

In general how well this works tends to depend on how close the curve is to the estimated mean.

Maybe using a penalty parameter - controling the strength of the regularization - is also thinkable? Something like this:

$$ \hat\theta_q(\lambda) = V ( V^H B^T B V + \lambda \cdot \Lambda^{-1})^{-1} V^H B^T q$$

**Thought**: A normal prior is probably not appropriate, because while its true that $\mathbb{E}[\theta] = 0$ (due to rotational symmetry) I would think that in general $\mathbb{E}[\lVert\theta\rVert] \neq 0$. A distribution more like a Normal distribution lying on a ring around 0 with radius $r = \lVert\theta\rVert$ is probably better? Then estimates wouldn't be pressed to zero so strongly, and instead would be pressed to curve of size equal to the mean.

\section{Proofs and Derivations}

\subsection{Derivation of Lemma \ref{lem:dist}}
\label{app:deriv-full-proc-dist}
\textbf{[TODO: Make dependence on $\gamma$ explicit!]}
\begin{proof}
  Start with ii.).
  Let $\widetilde z_2 = (\gamma, z_2) = (z_2 \circ \gamma) \sqrt{\dot\gamma}$, then optimise $d_{EF}([\beta_1],[\beta_2])^2$ over rotation and scaling, keeping $\gamma$ fixed.
  \begin{align*}
    d_{EF}([\beta_1],[\beta_2])^2 
      = & \inf_{\lambda \in \mathbb{R}^+,\, \theta \in [0,2\pi],\, \gamma \in \Gamma} \norm{z_1 - \lambda e^{i\theta} \widetilde z_2}^2 \\
      = & \inf_{\gamma \in \Gamma} \left( \inf_{\lambda \in \mathbb{R}^+,\, \theta \in [0,2\pi]} \langle z_1 - \lambda e^{i\theta} \widetilde z_2, z_1 - \lambda e^{i\theta} \widetilde z_2 \rangle \right) \\
      = & \inf_{\gamma \in \Gamma} \left( \inf_{\lambda \in \mathbb{R}^+,\, \theta \in [0,2\pi]} \norm{z_1}^2 + \lambda^2 \norm{\widetilde z_2}^2 - \lambda ( e^{i\theta} \langle z_1, \widetilde z_2 \rangle + e^{-i\theta} \langle \widetilde z_2, z_1 \rangle ) \right) 
  \end{align*}
  As $\langle z_1, \widetilde z_2 \rangle \in \mathbb{C}$, define $\langle z_1, \widetilde z_2 \rangle = \kappa_\gamma e^{i\phi_\gamma}$, where $(\cdot)_\gamma$ denotes the dependence on $\gamma$. Furthermore, use $\norm{z_{1,2}} = 1$, which implies $\norm{\widetilde z_2} = 1$, as re-parametrisation is norm-preserving, when using the elastic metric.
  \begin{align*}
    \dots\quad
     = & \inf_{\gamma \in \Gamma} \left( \inf_{\lambda \in \mathbb{R}^+,\, \theta \in [0,2\pi]} 1 + \lambda^2  - \lambda ( e^{i\theta} \kappa_\gamma e^{i\phi_\gamma} + e^{-i\theta} \kappa_\gamma e^{-i\phi_\gamma} ) \right) \\
     = & \inf_{\gamma \in \Gamma} \left( \inf_{\lambda \in \mathbb{R}^+,\, \theta \in [0,2\pi]} 1 + \lambda^2  - \lambda \kappa_\gamma \left( e^{i(\theta + \phi_\gamma)} + e^{-i (\theta + \phi_\gamma)} \right) \right) \\
     = &  \inf_{\gamma \in \Gamma} \left( \inf_{\lambda \in \mathbb{R}^+} 1 + \lambda^2  - \sup_{\theta \in [0,2\pi]} 2 \lambda \kappa_\gamma \cos{(\theta + \phi_\gamma)} \right) \\
     \overset{\theta = -\phi_\gamma}{=} & \inf_{\gamma \in \Gamma} \left( \inf_{\lambda \in \mathbb{R}^+} 1 + \lambda^2  - 2 \lambda \kappa_\gamma \right) 
  \end{align*}
  From $\frac{\partial}{\partial\lambda}\left( 1 + \lambda^2 - 2\lambda\kappa_\gamma \right) = 2\lambda - 2\kappa_\gamma \overset{!}{=} 0$ it follows that $\lambda = \kappa_\gamma$.
  $$\dots\quad = \inf_{\gamma \in \Gamma} \left( 1 + \kappa^2_\gamma  - 2 \kappa^2_\gamma \right) = \inf_{\gamma \in \Gamma} \left( 1 - \kappa^2_\gamma \right)$$
  \textbf{[TODO: Make this part clearer!]}
  Lemma \ref{lem:dist} ii.) follows from $\kappa^2_\gamma = \abs{\langle z_1, \widetilde z_2 \rangle}^2 = \langle z_1, \widetilde z_2 \rangle \langle \widetilde z_2, z_1 \rangle$ and $\widetilde z_2 = (z_2 \circ \gamma) \sqrt{\dot\gamma}$.
  Lemma \ref{lem:dist} i.) follows from $\lambda e^{i\theta} = \kappa_\gamma e^{-i\phi_\gamma} = \overline{\langle z_1, \widetilde z_2 \rangle}$.
\end{proof}


\section{Discussion of Possible Extensions to Closed Curves}
\label{app:closed}


\section{Shape-Smoothing Using the Estimated Covariance-Surface}
\label{app:smooth}
Idea: Smooth $q$ in mean basis $b(t)$, so that $\hat{q}(t) = b(t)^T \hat{\theta}_q$. Then the scalar products simplify to
$$ \langle q, \hat{p} \rangle \approx \hat{\theta}_q^H G \hat{\theta} $$
$$ \langle q, q \rangle \approx \hat{\theta}_q^H G \hat{\theta}_q $$

As $q$ can be sparse, we may want to use the estimated covariance matrix $\hat{\Xi}$ when estimating $\theta_q$ in the form of a Normal prior $\theta_q \sim \mathcal{N}_{\mathbb{C}^k}(0, \hat\Xi)$ leading to

$$ \hat\theta_q = (B^T B + \Xi^{-1})^{-1} B^T q $$

or equivalently using eigendecompositon $\Xi = V \Lambda V^{-1}$ with $V^H V = \mathcal{I}$ and $\Lambda = diag(\lambda_1, \dots, \lambda_k)$

$$ \hat\theta_q = V ( V^H B^T B V + \Lambda^{-1})^{-1} V^H B^T q$$

In general how well this works tends to depend on how close the curve is to the estimated mean.

Maybe using a penalty parameter - controling the strength of the regularization - is also thinkable? Something like this:

$$ \hat\theta_q(\lambda) = V ( V^H B^T B V + \lambda \cdot \Lambda^{-1})^{-1} V^H B^T q$$

**Thought**: A normal prior is probably not appropriate, because while its true that $\mathbb{E}[\theta] = 0$ (due to rotational symmetry) I would think that in general $\mathbb{E}[\lVert\theta\rVert] \neq 0$. A distribution more like a Normal distribution lying on a ring around 0 with radius $r = \lVert\theta\rVert$ is probably better? Then estimates wouldn't be pressed to zero so strongly, and instead would be pressed to curve of size equal to the mean.

\label{app:a}

\section{Additional Proofs and Derivations}
\label{app:a-deriv}

\subsection{Derivation of Lemma \ref{lem:2-fpdist}}
\label{app:a-deriv-fpdist}
\begin{align*}
  d_{FP}([\beta_1]_\Eucl,[\beta_2]_\Eucl)^2 
    = & \min_{\omega \in \mathbb{C}} \,\, \norm{\widetilde \beta_1 - \omega \widetilde \beta_2}^2 \\
    = & \min_{\lambda \in \mathbb{R}^+,\, \theta \in [0,2\pi)} \norm{\widetilde \beta_1 - \lambda e^{\iu\theta} \widetilde \beta_2}^2 \\
    = & \min_{\lambda \in \mathbb{R}^+,\, \theta \in [0,2\pi)} \langle \widetilde \beta_1 - \lambda e^{\iu\theta} \widetilde \beta_2, \widetilde \beta_1 - \lambda e^{\iu\theta} \widetilde \beta_2 \rangle \\
    = & \min_{\lambda \in \mathbb{R}^+,\, \theta \in [0,2\pi)} \norm{\widetilde \beta_1}^2 + \lambda^2 \norm{\widetilde \beta_2}^2 - \lambda ( e^{\iu\theta} \langle \widetilde \beta_1, \widetilde \beta_2 \rangle + e^{-\iu\theta} \langle \widetilde \beta_2, \widetilde \beta_1 \rangle 
\end{align*}
Define $\langle \widetilde \beta_1, \widetilde \beta_2 \rangle = \kappa e^{\iu\phi} \in \mathbb{C}$ with $\kappa \in \mathbb{R}^+$, $\phi \in [0,2\pi)$ and use $\norm{\widetilde \beta_1} = \norm{\widetilde \beta_2} = 1$.
\begin{align*}
  d_{FP}([\beta_1]_\Eucl,[\beta_2]_\Eucl)^2 
   = &  \min_{\lambda \in \mathbb{R}^+,\, \theta \in [0,2\pi)} 1 + \lambda^2  - \lambda ( e^{\iu\theta} \kappa e^{\iu\phi} + e^{-\iu\theta} \kappa e^{-\iu\phi} ) \\
   = &  \min_{\lambda \in \mathbb{R}^+,\, \theta \in [0,2\pi)} 1 + \lambda^2  - \lambda \kappa \left( e^{\iu(\theta + \phi)} + e^{-\iu (\theta + \phi)} \right) \\
   = &  \min_{\lambda \in \mathbb{R}^+} 1 + \lambda^2  - \max_{\theta \in [0,2\pi)} 2 \lambda \kappa \cos{(\theta + \phi)} \\
   \overset{\theta^\mathrm{opt} = -\phi}{=} & \min_{\lambda \in \mathbb{R}^+} 1 + \lambda^2  - 2 \lambda \kappa
\end{align*}
From $\frac{\partial}{\partial\lambda}\left( 1 + \lambda^2 - 2\lambda\kappa \right) = 2\lambda - 2\kappa \overset{!}{=} 0$ it follows that $\lambda^\mathrm{opt} = \kappa$.
\begin{align*}
  d_{FP}([\beta_1]_\Eucl,[\beta_2]_\Eucl)^2 
   = &  \left( 1 + \kappa^2  - 2 \kappa^2 \right) =  \left( 1 - \kappa^2 \right)
\end{align*}
\cref{lem:2-fpdist} i.) follows by considering $\kappa^2 = \abs{\langle \widetilde \beta_1, \widetilde \beta_2 \rangle}^2 = \langle \widetilde \beta_1, \widetilde \beta_2 \rangle \langle \widetilde \beta_2, \widetilde \beta_1 \rangle$.
Then
\begin{align*}
  d_{FP}([\beta_1]_\Eucl,[\beta_2]_\Eucl)
   = &  \sqrt{ 1 - \langle \widetilde \beta_1, \widetilde \beta_2 \rangle \langle \widetilde \beta_2, \widetilde \beta_1 \rangle }\,.
\end{align*}
\cref{lem:2-fpdist} ii.) follows by $\omega^\mathrm{opt} = \lambda^\mathrm{opt} e^{\iu \theta^\mathrm{opt}} = \kappa e^{-\iu\phi} = \overline{\langle \widetilde \beta_1, \widetilde \beta_2 \rangle} = \langle \widetilde \beta_2, \widetilde \beta_1 \rangle$.


\section{Shape-Smoothing Using the Estimated Covariance-Surface}
\label{app:a-smooth}
Instead of treating $\widetilde q_i^{(k)}$ as piecewise constant,  in mean basis $b(t)$, so that $\hat{q}(t) = b(t)^T \hat{\theta}_q$. Then the scalar products simplify to
$$ \langle q, \hat{p} \rangle \approx \hat{\theta}_q^H G \hat{\theta} $$
$$ \langle q, q \rangle \approx \hat{\theta}_q^H G \hat{\theta}_q $$

As $q$ can be sparse, we may want to use the estimated covariance matrix $\hat{\Xi}$ when estimating $\theta_q$ in the form of a Normal prior $\theta_q \sim \mathcal{N}_{\mathbb{C}^k}(0, \hat\Xi)$ leading to

$$ \hat\theta_q = (B^T B + \Xi^{-1})^{-1} B^T q $$

or equivalently using eigendecompositon $\Xi = V \Lambda V^{-1}$ with $V^H V = \mathcal{I}$ and $\Lambda = diag(\lambda_1, \dots, \lambda_k)$

$$ \hat\theta_q = V ( V^H B^T B V + \Lambda^{-1})^{-1} V^H B^T q$$

In general how well this works tends to depend on how close the curve is to the estimated mean.

Maybe using a penalty parameter - controling the strength of the regularization - is also thinkable? Something like this:

$$ \hat\theta_q(\lambda) = V ( V^H B^T B V + \lambda \cdot \Lambda^{-1})^{-1} V^H B^T q$$

**Thought**: A normal prior is probably not appropriate, because while its true that $\mathbb{E}[\theta] = 0$ (due to rotational symmetry) I would think that in general $\mathbb{E}[\lVert\theta\rVert] \neq 0$. A distribution more like a Normal distribution lying on a ring around 0 with radius $r = \lVert\theta\rVert$ is probably better? Then estimates wouldn't be pressed to zero so strongly, and instead would be pressed to curve of size equal to the mean.


\section{Removing the Iteration over Rotation}
\label{app:a-warping}
\todo[inline]{As suggested to Lisa}
Ich habe mich noch gefragt ob man anstatt Iteration über Eq.\ \ref{eq:argmin_rot} und Eq.\ \ref{eq:argmin_wrp} die Distanz in Lemma \ref{lem:dist} ii.) auch direkter optimieren kann.
\begin{align*}
  \gamma^* = & \argmin_{\gamma \in \Gamma} \sqrt{1 - \langle z_1, \widetilde z_2 \rangle \langle \widetilde z_2, z_1 \rangle } \\
    = & \argmax_{\gamma \in \Gamma} \langle z_1, \widetilde z_2 \rangle \langle \widetilde z_2, z_1 \rangle \\
    = & \argmax_{\gamma \in \Gamma} \int_0^1 \int_0^1
    \overline{z_1(t)} \underbrace{\widetilde z_2(t) \overline{\widetilde z_2(s)}}_{\coloneqq \, {\widetilde C}(s,t)} z_1(s) \, dt ds \\
    = & \argmax_{\gamma \in \Gamma} \langle \widetilde C \, z_1, z_1 \rangle
\end{align*}
Mit $\widetilde C(s,t)$ der Kovarianz-Funktion von $\widetilde z_2 = (z_2 \circ \gamma) \sqrt{\dot\gamma}$.
Habe das jetzt nicht weiter verfolgt (weil Zeit), aber vielleicht ist das ganz interessant?
Man kann das glaube ich auch umschreiben mit $\breve z_1 = (z_1 \circ \gamma^{-1}) \sqrt{\dot{\gamma^{-1}}}$ als $\argmax_{\gamma \in \Gamma} \langle C \, \breve z_1, \breve z_1 \rangle$.

\section{Analysis of Tounge Shape Variability using Procrustes Residuals}
\begin{itemize}
  \item Want investiage how the effect of the flanking vowels \enquote*{aa} and \enquote*{ii} on tounge shape differs for the consonants 'd', 'l', 'n' and 's', while controlling for variability over six speakers and 5--7 repetitions of the same word per speaker.
  \item Analysis should be invariant with respect to size, rotation, translation and parameterisation of the tounges.
    This can e.g.\ account for anatomical differences between speakers, or inaccuracies in the measurement process.
  \item \textbf{Idea:} Estimate a global mean function $q_0(t)$ from all curves and calculate the elastic full Procrustes residuals.
    They are given by the difference of elastic full Procrustes fits and the mean function on SRV level
    $$ \hat r_i = \left( \omega_i^\mathrm{opt} \cdot \widetilde q_i \circ \gamma_i^\mathrm{opt} \right) \sqrt{\dot\gamma_i^\mathrm{opt}} - \hat q_0 \,.$$
    We can perform analysis over the $\hat r_i(t)$. 
  \item A word is a combination of vowels and consonants $(C_v, C_c)$ with $C_v \in \{\mathrm{aa}, \mathrm{ii}\}$ and $C_c \in \{\mathrm{d},\mathrm{l},\mathrm{n},\mathrm{s}\}$, e.g.\ \enquote*{pada} corresponds to $(\mathrm{aa},\mathrm{d})$ as all words start with 'p'.
    We can index each word--speaker pair $\left( C_v, C_c, C_s \right)$ by $k$, where $C_s \in \{1,\dots,6\}$ and $k = 1, \dots, K$ with $K = 2 \cdot 4 \cdot 6 = 48$.
  \item Then we can gather the residuals $\hat r_{(k, rep_k)}(s_{(k,rep_k,j)})$ of the repetitions $rep_k$ (their number varies with $k$) of each word--speaker pair $k$, and estimate a \enquote*{mean} residual function for that word and speaker:
    $$ \hat r_k (t) = \hat r_{\left( C_v, C_c, C_s \right)} (t) \approx \hat \theta_k^\top b(t)$$
    Atm I use the mean basis here, which means linear B-Splines. Might be nicer to have something that is smoother.
  \item We are interested in the effect of the flanking vowels on tounge shape given, for each consonant-speaker pair, by this difference:
    $$ \hat d_{(C_c, C_s)}(t) = \hat r_{(\mathrm{aa}, C_c, C_s)} (t) - \hat r_{(\mathrm{ii}, C_c, C_s)} (t) = 
      \left( \hat\theta^\top_{(\mathrm{aa}, C_c, C_s)} - \hat\theta^\top_{(\mathrm{aa}, C_c, C_s)} \right) b(t)\,. $$
    Note that $\hat d(t) : [0,1] \rightarrow \mathbb{C}$ (two-dimensional)! The absolute differences over speakers and consonants are given by $\abs{\hat d_{(C_c, C_s)}(t)}$ are plotted in \cref{fig:4-diffs-vpn}.
\end{itemize}
\begin{figure}
  \centering
  \inputTikz{4-diffs-vpn}
  \caption{a caption}
  \label{fig:4-diffs-vpn}
\end{figure}
\begin{itemize}
  \item Finally we can average the differences over each speaker as
    $$ \hat d_{(C_c)} (t) = \left(\frac{1}{6} \sum_{C_s} \hat \theta^\top_{(C_c, C_s)} \right) b(t)$$
    or likewise as absolute difference  $\abs{\hat d_{(C_c)}(t)}$ plotted in \cref{fig:4-diffs}.
\end{itemize}
\begin{figure}
  \centering
  \inputTikz{4-diffs}
  \caption{Another caption}
  \label{fig:4-diffs}
\end{figure}
\begin{itemize}
  \item Man sieht, dass der Einfluss des flankierenden Vokals unabhängig vom Konsanten zu sein scheint.
    Das ist zumindest ein anderes Ergebniss, als in dem Consulting Projekt.
  \item Die Analysie ist hier auf SRV level.
  \item Sofern das alles so Sinn macht, könnte man jetzt noch Konfidenzintervalle undsowas dazu berechnen.
\end{itemize}
